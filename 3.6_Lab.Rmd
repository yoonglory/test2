---
title: "linear test"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
```

# chapter 3.6 Linear Regression

## 3.6.1 Libraries

### 먼저 필요한 패키지를 설치한다.

```{r}
library(MASS)
library(ISLR)
```

## 3.6.2 Simple Linear Regression

### 내장 데이터 셋을 이용하여 간단한 회귀분석을 해본다.

```{r}
names(Boston)
```
response: medv
predictor: lstat

```{r}
lm.fit=lm(medv~lstat, data=Boston)
```
결과값을 볼까?
```{r}
lm.fit
summary(lm.fit)

names(lm.fit)
coef(lm.fit)

confint(lm.fit)
```

이 모델로 예측해본다.
```{r}

predict(lm.fit, data.frame(lstat=c(5,10,15)),interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)),interval="prediction")
```
설정된 범위는 다르지만 fit 된 값은 동일하다. 
prediction의 범위가 훨씬 더 넓다.

이제 그래프를 그려본다.
```{r, include=FALSE}
plot(Boston$lstat,Boston$medv)
abline(lm.fit)
```
그래프를 꾸며보자.
```{r, include=FALSE}
plot(Boston$lstat,Boston$medv)
abline(lm.fit,lwd=3,col='red')
```

혹은
```{r}
plot(Boston$lstat,Boston$medv,col='red')
```
혹은
```{r}
plot(Boston$lstat,Boston$medv,pch=20)
```
혹은
```{r}
plot(Boston$lstat,Boston$medv,pch='+')
```
혹은
```{r}
plot(Boston$lstat,Boston$medv,pch=1:20)
```

**이제 lm 그래프**
```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```
따로 한번 살펴보자.
```{r}
plot(predict(lm.fit),residuals(lm.fit))
plot(predict(lm.fit),rstudent(lm.fit))
```

잔차그래프를 살펴보니 lastat와 medv 사이는 non-linear 함을 알 수 있다.
Leverage statistics를 사용해보자.
```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```


## 3.6.3 multiple Linear Regression

소심하게 변수 하나를 추가해서 모델을 만들어보자.
```{r}
lm.fit.2=lm(medv~lstat+age, data=Boston)
summary(lm.fit.2)
```

모든 변수를 다 고려해서 모델을 만들어보자.
```{r}
lm.fit.2.all=lm(medv~.,data=Boston)
summary(lm.fit.2.all)
```

vif 함수 사용을 위해 car 패키지를 설치한다.
```{r}
library(car)
vif(lm.fit.2.all)
```
불필요한 변수를 제외한다.
```{r}
lm.fit.3=lm(medv~.-age,data=Boston)
summary(lm.fit.3)
```

## 3.6.4 Interaction Terms

interaction term을 포함하여 모델을 만들어보자.
```{r}
summary(lm(medv~lstat*age,data=Boston))
```

### 3.6.5 Non-linear Transformations of the Predictors

```{r}
lm.fit.4=lm(medv~lstat+I(lstat^2),data=Boston)
summary(lm.fit.4)
```

보아하니 모델의 성능이 더 좋아졌다.
이제 2차항이 선형보다 우수한 정도를 정량화 해본다.

```{r}
lm.fit=lm(medv~lstat,data=Boston)
anova(lm.fit,lm.fit.4)
```
두 모델간에 성능 차이가 있는지 없는지에 대한 가설 검정을 진행했다.
p-value와 F-statistics를 보니 2차항이 포함된 모델이 성능이 더 좋다.

그래프로 확인해보자.
```{r}
par(mfrow=c(2,2))
plot(lm.fit.4)
```

모양이 약간 그러하다.
다항식 모델을 한번 적용해보자.

```{r}
lm.fit.5=lm(medv~poly(lstat,5),data=Boston)
summary(lm.fit.5)
```
이전의 2차항만을 포함했던 모델보다 성능이 좋아졌다.
그러나 일부 5차항 식에서 유의미하지 않은 p-value를 얻었다.

꼭 다항식으로만이 아니라 로그 변환도 가능하다.
```{r}
summary(lm(medv~log(rm),data=Boston))
```

## 3.6.6 Qualitative Predictors

양적, 질적 변수가 포함된 새로운 데이터 셋을 불러온다.
```{r}
names(Carseats)
```

질적변수인 shelveloc를 이용해보자.
3가지 요인을 가지는 factor다.
R은 이 변수를 사용할 때, 자동으로 더미변수를 생성한다. (0,1,2)

```{r}
lm.fit.6=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit.6)
```
정말로 R이 shelveloc 변수에 대한 더미변수를 자동으로 생성했는지 확인해보자.
```{r}
contrasts(Carseats$ShelveLoc)
```


이걸로 테스트 끄읕!

